<!DOCTYPE html>

<html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<head>
    <title>Linear regression</title>
    <link href="styles.css" rel="stylesheet" />
    <link rel="icon" type="image/x-icon" href="/favicons/fav_light.png" media="(prefers-color-scheme:light)">
    <link rel="icon" type="image/x-icon" href="/favicons/fav_dark.png" media="(prefers-color-scheme:dark)">
</head>
<script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]},
    "HTML-CSS": { linebreaks: { automatic: true }, mtextFontInherit: true },
    CommonHTML: { linebreaks: { automatic: true }, mtextFontInherit: true },
    SVG: { linebreaks: { automatic: true }, mtextFontInherit: true }
  });
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script>


<body onresize="resizeCanvas();" id="body_id">

     <ul class="customnav">
      <li class="home"><a href="/index">Home</a></li>
      <li><a class="active_tab" href="/linear_regression">Linear regression</a></li>
      <li><a href="/internet_movie_dataviz">Internet Movie Dataviz</a></li>
      <li><a href="/game_of_life">Game of Life</a></li>
      <li><a href="/kia_picanto_go_fast">Kia Picanto go fast</a></li>
    </ul>

    <h1 id="linear-regression">Linear regression</h1>
    <h3 id="_26-09-22_"><em>20-01-23</em></h3>

<!-- directives:[] -->
<p>When analysing a dataset, it is useful to be able to say something about interaction between variables. A regression analysis can be used to estimate such a relationship. Such an analysis can be achieved using machine learning. Many packages are available for any programming language to perform linear regression. I decided to write one myself in Python.</p>
<p>Say we have data on housing prices, and the room area these houses offer. We can plot this data as follows:</p>
<p><img src="linear_regression_images/data_plot 2.png"></p>
<p>Can we say something about how well the <strong>area</strong> (independent value) can influence the <strong>price</strong> (dependent value)?</p>
<p>To begin, we can draw a random straight line through the data. We draw this such that $y = ax + b$. In machine learning the <em>a</em> here is also called the <strong>weight</strong>, and the <em>b</em> is called the <strong>bias</strong>. Thus, for each value $x$, a $\hat{y}$ value is predicted. Say we start our line <em>500 000</em> (the bias), and we take a slope of <em>1000</em> (the weight). For every square meter we add to the area, we expect the price to increase by <em>1000</em>.</p>
<pre><code class="prettyprint"># For every x value in the dataset, we can predict a y value using our weight and bias values. By using Numpy this calculation is done for every value automatically.

def predict_y(x: NPArray, weight: float, bias: float) -&gt; NPArray:
    return weight * x + bias</code></pre>
<p><img src="linear_regression_images/random_line 1.png"></p>
<h1 id="the-loss-function">The loss function</h1>
<p>How well does this line fit our data? We can determine this by calculating how far the real values $y$ lay from our approximated values $\hat{y}$ using the line formula. This is called the loss function: $(ŷ - y)^2$. Do this for every data point, and we have a measure of fit: $\frac{1}{2m}\sum\limits_{i=1}^{m} (ŷ_i - y_i)^2$.</p>
<pre><code class="prettyprint">def calc_loss(y: NPArray, y_hat: NPArray) -&gt; float:
    loss_array = (y_hat - y) ** 2
    loss: float = np.add.reduce(loss_array) * (1 / (2 * Const.M))
    return loss</code></pre>
<p>Using the loss, we can derive formulas to adjust the weights and biases. This is done by taking the derivative of the loss function above for both.<br />
For the weight: $w_{new} = w - \alpha\frac{1}{m}\sum\limits_{i=1}^{m} (wx_i + b - y_i)x_i$. <br />
For the bias: $b_{new} = b - \alpha\frac{1}{m}\sum\limits_{i=1}^{m} (wx_i + b - y_i)$. <br />
Alpha ($\alpha$) represents the learning rate you want to set. A smaller rate takes longer to compute, but too large a value can result in inaccurate results.</p>
<pre><code class="prettyprint">def update_weight(weight: float, bias: float, x: NPArray, y: NPArray) -&gt; float:
    der_array = (weight * x + bias - y) * x
    der = np.add.reduce(der_array) * (1 / Const.M)

    weight_new: float = weight - Const.ALPHA * der
    return weight_new</code></pre>
<p>Repeatedly adjusting the weight and bias <em>1000</em> times results in the following line:</p>
<p><img src="linear_regression_images/Linear_regression 1.png"><br />
Weight: <em>4620</em>, bias: <em>238 733</em>.</p>
<p>If we plot the loss measure through time we can see as the algorithms repeatedly adjusts the weight and bias (below). The graph converges to the most optimal fit. For this dataset and learning rate, we can see after about <em>200</em> runs the loss value starts to flatten. We are now approaching the most optimal line as plotted above.</p>
<p><img src="linear_regression_images/loss 1.png"></p>
<h1 id="gradient-descent">Gradient descent</h1>
<p>We can also plot this in 3D for a wide variety of possible weights and biases. From whatever starting point (red cross), with enough runs the weight and bias will be adjusted and the loss will decrease accordingly. This is presented as the red line in the graph. Imagine a ball rolling down the slope. This process is called gradient descent. The lowest point on the graph represents the optimal weight and bias we found above.</p>
<p><img src="linear_regression_images/gradient descent.png"></p>
<h1 id="rsup2sup-and-p-values">R<sup>2</sup> and p-values</h1>
<p>One form of the loss value that is often reported in a regression analysis is called the R<sup>2</sup> value. This is a measure between 0 and 1 on how well the line fits our data. If all data points were to lay on the predicted line, the R would be 1. In our dataset, the R<sup>2</sup> value is: <em>0.29</em>. In other words, 29% of our data can be explained by the line we drew through it.</p>
<pre><code class="prettyprint"># The R-squared value can be calculated using the sum of squares total, and sum of squares regression values.

def calc_r_squared(y: NPArray, y_hat: NPArray) -&gt; float:
    sst = np.add.reduce((y-mean(y))**2)
    ssr = np.add.reduce((y_hat-mean(y))**2)

    r_squared = ssr/sst
    return r_squared</code></pre>
<p>Another often used value that is reported is the p-value of the slope. A statistically significant value shows it's likely the slope (weight) is different from zero. In other words, whether there actually exists a relationship between the independent and dependent variables.<br />
In our dataset, the p-value is: <em>7.40e-42</em>. So very significant.</p>
<pre><code class="prettyprint"># The standard error can be calculated using the sum of squares error, the  sum of squares total for x, and the degrees of freedom.
# The t-value can be calculated by dividing the slope by the standard error.
# This value can then be converted to the p-value.

def calc_p_value(y_hat: NPArray, y: NPArray, x: NPArray, weight: float) -&gt; float:
    dof = Const.M - 2
    sse = np.add.reduce((y_hat - y) ** 2)
    x_sst = np.add.reduce((x - mean(x)) ** 2)
    se = sqrt((1 / dof) * (sse / x_sst))
    t_val = weight / se
    p_value: float = t.sf(abs(t_val), dof) * 2  # *2 for two-tailed p-value

    return p_value</code></pre>
<p>Comparing our results to an often used python package <em>Scipy.stats</em> reveals my script to function well.</p>
<p>My script:<br />
Weight: <em>4619.69</em><br />
Bias: <em>238 733.85</em><br />
R<sup>2</sup>: <em>0.2872</em><br />
p-value: <em>7.4026e-42</em><br />
nr. of cycles ran: <em>5000</em></p>
<p>Scipy.stats:<br />
LinregressResult(slope=<em>4619.74</em>, intercept=<em>238 730.84</em>)<br />
Scipy.stats r-squared: <em>0.2872</em><br />
Scipy.stats p-value: <em>7.3882e-42</em></p>
<h1 id="speeding-up-numpy-and-normalization-of-data">Speeding up: Numpy and normalization of data</h1>
<p>One roadblock in machine learning is the large number of computations that have to be performed. Luckily, we can help the computer via good design of how it handles these calculations. One such aid is found in the popular python package Numpy. Here, computations are performed via vectorization, something the cpu of a computer rather adept at. Using this, calculations across an array can be performed simultaneously and in bulk. The usage of Numpy speeds up certain applications by multiple orders of magnitude.</p>
<p>Computers don't work well with large numbers. Calculation quickly get incredibly large, such as when calculating the loss value using multiplication. To avoid this, we can perform a normalization of the data. For instance, the data in this script was normalized using min-max normalization. Here, the <em>x</em> and <em>y</em> values get normalized between 0 and 1.</p>
<pre><code class="prettyprint"># We calculate the upper and lower values of x and y and save them for later.

def normalize_data(x: NPArray, y: NPArray, weight: float, bias: float) -&gt; tuple[NPArray, NPArray]:
    Nor.y_min, Nor.y_max = min(y), max(y)
    Nor.x_min, Nor.x_max = min(x), max(x)

    x_nor = (x - Nor.x_min) / (Nor.x_max - Nor.x_min)
    y_nor = (y - Nor.y_min) / (Nor.y_max - Nor.y_min)
    return x_nor, y_nor</code></pre>
<p>Finally, we can denormalize the data to return the corresponding values in original scale.</p>
<pre><code class="prettyprint"># The calculated bounds of before are used again here.

def denormalize_data(x_nor: NPArray, y_nor: NPArray, y_hat: NPArray, weight: float, bias: float) -&gt; tuple[NPNum, NPNum, NPNum, float, float]:
    x = x_nor * (Nor.x_max - Nor.x_min) + Nor.x_min
    y = y_nor * (Nor.y_max - Nor.y_min) + Nor.y_min
    y_hat = y_hat * (Nor.y_max - Nor.y_min) + Nor.y_min
    return x, y, y_hat</code></pre>

    <span style="font-size: small; ">
    This code was written in Python. The packages Numpy and Matplotlib were used. Styling and control buttons were written in HTML and css.
    </span>

        <div class="github-menu"> <a target="_blank" rel="noopener noreferrer" href="https://github.com/technologiemas/game_of_life">
                <div class="github-element">
                    <svg xmlns="http://www.w3.org/2000/svg" width="41.6" height="44.8" viewBox="0 0 416 448" id="github"><path fill="#53b292" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"></path></svg>
                    <div class="github-text">
                        <h5>GitHub Page</h5>
                    </div>
                </div>
        </a>
        </div>

    <br><br><br>


</body>
</html>